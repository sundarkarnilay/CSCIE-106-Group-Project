---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "EXT CSCI E-106 Model Data Class Special Project Template"
author:
- Nilay Sundarkar
- Author Two
- Author Three
- Author Four

date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## Data

Refer to the **Housing prices in Ames, Iowa**

```         
2930 observations, 82 variables
```

```{r}
#Step 0: Data Preparations
#install.packages("visdat")
library(readr)
library(visdat)
library(tidyr)
# Checking for NA, or missing data using graphics 
ames_data <- read_csv("ames.csv")
vis_miss(ames_data)
str(ames_data)
```

## **Description**

Data set contains information from the Ames Assessor's Office used in
computing assessed values for individual residential properties sold in
Ames, IA from 2006 to 2010. See
[here](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) for
detailed variable descriptions.

## **Objective**

Using the data build a prediction model using explanatory variables or
predictors to allow a typical buyer or real estate agent to sit down and
estimate the selling price of a house "SalePrice" (It is a continuous
variable) is the response variable.

## Due Date: May 6, 2024 at 11:59 pm EST

## **Instructions:**

|     |     |                                                                                                                                                                                                                                                           |
|-------------------|-------------------|----------------------------------|
| 1   |     | Join a team with your fellow students with appropriate size (at most four students total). You may post an advertising in ED. Once you are set, send to rafael_gomeztagle\@g.harvard.edu the name of the team members and their emails.                   |
| 2   |     | Review the dataset named "ames'csv, report on preliminary findings (missing data, type of variables, distributions).                                                                                                                                      |
| 3   |     | Create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set.                                                                                                                           |
| 4   |     | Investigate the data and combine the level of categorical variables if needed and drop variables as needed. For example, you may drop id, variables with too many missing observations, etc.                                                              |
| 5   |     | Create scatter plots and a correlation matrix for the train data set. Interpret the possible relationship between the response and the covariates.                                                                                                        |
| 6   |     | Build several multiple linear models by using the stepwise selection methods. Compare the performance of the best two linear models.                                                                                                                      |
| 7   |     | Make sure that model assumption(s) are checked for the final model. Apply remedy measures (transformation, etc.) that helps satisfy the linear model assumptions.                                                                                         |
| 8   |     | Investigate unequal variances and multicollinearity. If necessary, apply remedial methods (WLS, Ridge, Elastic Net, Lasso, etc.).                                                                                                                         |
| 9   |     | Build an alternative model based on one of the following approaches to predict price: regression tree, NN, or SVM. Then check again the applicable model assumptions.                                                                                     |
| 10  |     | Use the test data set to assess the model performances from above.                                                                                                                                                                                        |
| 11  |     | Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model.                                                                                              |
| 12  |     | Create a model development document that describes the model following this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc.: be sure you populate all the sections of this template. |
| 13  |     | Each student must submit the files on Canvas to get the full credit.                                                                                                                                                                                      |

**Notes:** **No typographical errors, grammar mistakes, or misspelled
words, use English language** **All tables need to be numbered and
describe their content in the body of the document** **All
figures/graphs need to be numbered and describe their content** **All
results must be accurate and clearly explained for a casual reviewer to
fully understand their purpose and impact** **Submit both the RMD
markdown file and PDF with the sections with appropriate explanations. A
more formal document in Word can be used in place of the pdf file but
must include all appropriate explanations.**

1.  Send email details - done by Simon

2.  Review the dataset named "ames'csv, report on preliminary findings
    (missing data, type of variables, distributions).

    Data set contains information from the Ames Assessor’s Office used
    in computing assessed values for individual residential properties
    sold in Ames, IA from 2006 to 2010.

    The data has 82 columns which include 23 nominal, 23 ordinal, 14
    discrete, and 20 continuous variables (and 2 additional observation
    identifiers).

3.  Create the train data set which contains 70% of the data and use
    set.seed (1023). The remaining 30% will be your test data set.

```{r}
ames.smp_size <- floor(0.70 * nrow(ames_data))
set.seed(1023)
ames.train_index <- sample(seq_len(nrow(ames_data)), size = ames.smp_size)
ames.train_data <- ames_data[ames.train_index, ]
ames.test_data <- ames_data[-ames.train_index, ]
```

4.  Investigate the data and combine the level of categorical variables
    if needed and drop variables as needed. For example, you may drop
    id, variables with too many missing observations, etc.

Dropping Order, PID as they are just an identifier for the observations.

Dropping "Pool.QC", "Misc.Feature", "Alley","Fence", "Fireplace.Qu", "Lot.Frontage","Garage.Yr.Blt", "Garage.Finish", "Garage.Qual","Garage.Cond", "Garage.Type" as they have high number of missing values.

Cleaning NA rows for the rest of the data.

```{r}
ames_data.df <- data.frame(ames_data)
# Domain analysis to clean up data
# We rely on descriptions/comments provided at https://jse.amstat.org/v19n3/decock/DataDocumentation.txt, statistical tests and our own intuition to determine if there are any columns that need to be removed
# The document mentions below for outliers - 
#SPECIAL NOTES:
#There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them #quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced #relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before #assigning it to students.
ames.known_outliers <- ames_data.df[ames_data.df$area > 4000,]
ames_data.df <- ames_data.df[!(ames_data.df$PID %in% ames.known_outliers$PID),]
ames.train_data.df <- data.frame(ames.train_data)
ames.train_data.df <- ames.train_data.df[!(ames.train_data.df$PID %in% ames.known_outliers$PID),]
ames.test_data.df <- data.frame(ames.test_data)
ames.test_data.df <- ames.test_data.df[!(ames.test_data.df$PID %in% ames.known_outliers$PID),]
```

```{r}
# check which columns have NA values and how many per column
na_count <-sapply(ames_data.df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count$name<-rownames(na_count)
na_count <- na_count[na_count$na_count>0,]
na_count <- data.frame(na_count)
na_count <- na_count[order(na_count$na_count, decreasing = TRUE), ]
na_count
# drop columns that have very high number of NA values and those that are not related to the response variable (order and PID)
# Misc.Feature is directly associated with Misc.Val - so dropping Misc.Val
drops <- c("Pool.QC", "Misc.Feature", "Alley","Fence", "Fireplace.Qu", "Order", "PID", "Misc.Val")
ames_data.df <- ames_data.df[ , !(names(ames_data.df) %in% drops)]
```

```{r}
#for the rest of columns that have somewhat high NA values, we remove the NA rows and check the correlation between that column and the response variable
`%notin%` <- Negate(`%in%`)
na_count <- na_count[na_count$name %notin% drops,]
na_count
# Lot.Frontage has 490 NA values
Lot.Frontage_df <- data.frame(ames_data$price,ames_data$Lot.Frontage)
Lot.Frontage_df <- drop_na(Lot.Frontage_df)
# low correlation between Lot.Frontage and SalePrice
cor(Lot.Frontage_df$ames_data.price,Lot.Frontage_df$ames_data.Lot.Frontage)
# Garage.Yr.Blt, Garage.Finish , Garage.Qual, Garage.Cond and Garage.Type are all related to Garage and seem to have NA values for the same rows
Garage_df <- data.frame(ames_data$price, ames_data$Garage.Yr.Blt, ames_data$Garage.Finish, ames_data$Garage.Qual, ames_data$Garage.Cond, ames_data$Garage.Type)
Garage_df <- drop_na(Garage_df)
# no significant correlation is observed with the response variable
plot(Garage_df)
drops <- c(drops, "Lot.Frontage","Garage.Yr.Blt", "Garage.Finish", "Garage.Qual","Garage.Cond", "Garage.Type")
ames_data.df <- ames_data.df[ , !(names(ames_data.df) %in% drops)]
ames.train_data.df <- ames.train_data.df[ , !(names(ames.train_data.df) %in% drops)] 
ames.test_data.df <- ames.test_data.df[ , !(names(ames.test_data.df) %in% drops)]
```

```{r}
na_count <- na_count[na_count$name %notin% drops,]
na_count
# the remaining columns that have any NA rows are low in number, so we will clean the data for those rows
ames_data.df <- drop_na(ames_data.df)
ames.train_data.df <- drop_na(ames.train_data.df)
ames.test_data.df <- drop_na(ames.test_data.df)
```

```{r}
# check for significance of categorical variables remaining
categoricalVarsColumnNames <- c("MS.SubClass", "MS.Zoning", "Street", "Lot.Shape", "Land.Contour", "Utilities", "Lot.Config", "Land.Slope", "Neighborhood", "Condition.1", "Condition.2", "Bldg.Type", "House.Style", "Overall.Qual", "Overall.Cond", "Year.Built", "Year.Remod.Add", "Roof.Style", "Roof.Matl", "Exterior.1st", "Exterior.2nd", "Mas.Vnr.Type", "Exter.Qual", "Exter.Cond", "Foundation", "Bsmt.Qual", "Bsmt.Cond", "Bsmt.Exposure", "BsmtFin.Type.1", "BsmtFin.Type.2", "Heating", "Heating.QC", "Central.Air", "Electrical","Bsmt.Full.Bath", "Bsmt.Half.Bath", "Full.Bath", "Half.Bath", "Bedroom.AbvGr", "Kitchen.AbvGr", "Kitchen.Qual", "TotRms.AbvGrd","Functional", "Fireplaces", "Garage.Cars", "Paved.Drive", "Mo.Sold" , "Yr.Sold", "Sale.Type", "Sale.Condition")

# generic function to test chi square test for a categorical variable
# here we calculate anova for a model with the categorical variable and another model without it
modelWithAllColumns <- glm(price~., data = ames.train_data.df)
chiTest <- function(columnName) {
  options(scipen = 999)
  ames.train_data.withoutColumnPassed <- ames.train_data.df[ , !(names(ames.train_data.df) %in% c(columnName))]
  modelWithoutColumnPassed <- glm(price~.,data = ames.train_data.withoutColumnPassed)
  aqq <- anova(modelWithAllColumns,modelWithoutColumnPassed,test="Chisq")
  return (aqq$`Pr(>Chi)`[2])
}

# data frame to hold results for each categorical variable
chiTestResults <- data.frame(matrix(ncol = 2, nrow = 0))
x <- c("Column Name", "ChiValue")
colnames(chiTestResults) <- x

# pass each categorical variable in the function
for (i in categoricalVarsColumnNames){
  chiResult <- chiTest(i)
  chiTestResults[nrow(chiTestResults) + 1,] = c(i,chiResult)
}
chiTestResults 
insignificantCategoricalColumns <- chiTestResults[chiTestResults$ChiValue > 0.05,]
insignificantCategoricalColumns

# Now looking at the insignificant categorical columns to see if we want to keep any that seem to be relevant or those that have a score close to 0.05
# retaining Year.Remod.Add, Roof.Style, Full.Bath, Kitchen.AbvGr, Yr.Sold, Sale.Condition

insignificantCategoricalColumns <- insignificantCategoricalColumns[insignificantCategoricalColumns$`Column Name` %notin% c("Year.Remod.Add", "Roof.Style", "Full.Bath", "Kitchen.AbvGr", "Yr.Sold", "Sale.Condition"),]
insignificantCategoricalColumns

# cleaning data for insignificant categorical columns
ames_data.df <- ames_data.df[ , !(names(ames_data.df) %in% insignificantCategoricalColumns$`Column Name`)]
ames.train_data.df <- ames.train_data.df[ , !(names(ames.train_data.df) %in% insignificantCategoricalColumns$`Column Name`)] 
ames.test_data.df <- ames.test_data.df[ , !(names(ames.test_data.df) %in% insignificantCategoricalColumns$`Column Name`)]
```


5.  Create scatter plots and a correlation matrix for the train data
    set. Interpret the possible relationship between the response and
    the covariates.

```{r}
categoricalVarsColumnNames.df<- data.frame(categoricalVarsColumnNames)
categoricalVarsColumnNames.df <- categoricalVarsColumnNames.df[categoricalVarsColumnNames.df$categoricalVarsColumnNames %notin% insignificantCategoricalColumns$`Column Name`,]

# function to factor numeric values for categorical variables
factorCatVars <- function(df) {
  c <- unique(df[name])
  levels <- c[,1]
  labels <- c()
  for (i in 1:nrow(c)) {
    labels <- append(labels, i)
  }
  df[,name] <- factor(df[,name],levels = levels,labels = labels)
  return (df)
}


for(name in categoricalVarsColumnNames.df){
  ames.train_data.df = factorCatVars(ames.train_data.df)
  ames.test_data.df = factorCatVars(ames.test_data.df)
}

predictorVars <- ames.train_data.df[ ,!(names(ames.train_data.df) %in% c("price"))]
par(mfrow=c(1,2))
for (i in 1:ncol(predictorVars)) {
  columnName <- colnames(predictorVars[i])
  mainText <- paste("Sale Price vs", columnName, sep=" ")
  plot(ames.train_data.df$price, predictorVars[,i],xlab = columnName, ylab = "Sale Price", main = mainText)
}

```
```{r}
library(ggplot2)
library(reshape2)
temp.df <- ames.train_data.df
for(name in categoricalVarsColumnNames.df){
  temp.df[,name] <- as.numeric(temp.df[,name])  
}
totalColumns <- ncol(temp.df)
step_size <- 10
startIndex <- 1
endIndex <- step_size

# function that will be called multiple times to print correlation matrices
printCorrelationMatrix <- function(start,end) {
  temp1.df <- data.frame(temp.df[start:end])
  tempColumnNames <- colnames(temp1.df)
  if("price" %notin% tempColumnNames){
    temp1.df$price <- temp.df$price
  }
  Correlations<-round(cor(temp1.df),2) 
  melted_cormat <- melt(Correlations)
  print(ggplot(data = melted_cormat, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    geom_text(aes(Var2, Var1, label = value), size = 2) +
    scale_fill_gradient2(low = "blue", high = "red", limit = c(-1, 1), name = "Correlation") +
    theme(axis.title.x = element_text(), axis.title.y = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), panel.background = element_blank()))
}
# end function

while(endIndex <= totalColumns){
  printCorrelationMatrix(startIndex,endIndex)
  startIndex <- startIndex + step_size
  endIndex <- endIndex + step_size
  if(endIndex > totalColumns & startIndex < totalColumns){
    printCorrelationMatrix(startIndex,totalColumns)
  }
}
```

6. Build several multiple linear models by using the stepwise selection methods. Compare the performance of the best two linear models.

7. Make sure that model assumption(s) are checked for the final model. Apply remedy measures (transformation, etc.) that helps satisfy the linear model assumptions.

8. Investigate unequal variances and multicollinearity. If necessary, apply remedial methods (WLS, Ridge, Elastic Net, Lasso, etc.). 

9. Build an alternative model based on one of the following approaches to predict price: regression tree, NN, or SVM. Then check again the applicable model assumptions.

10. Use the test data set to assess the model performances from above.

11. Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model. 

12. Create a model development document that describes the model following this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc.: be sure you populate all the sections of this template.

13. Each student must submit the files on Canvas to get the full credit. 

This data was taken from the Kaggle competition please click on the link
below for details:
<https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/rules>

|                                                                                                                                                                                                                                                                                                                   |
|------------------------------------------------------------------------|
| This section will describe the model usage, your conclusions and any regulatory and internal requirements. In a real world scenario, this section is for senior management who do not need to know the details. They need to know high level (the purpose of the model, limitations of the model and any issues). |

: ***Executive Summary***

## I. Introduction (5 points)

*This section needs to introduce the reader to the problem to be
resolved, the purpose, and the scope of the statistical testing applied.
What you are doing with your prediction? What is the purpose of the
model? What methods were trained on the data, how large is the test
sample, and how did you build the model?*

## I. Description of the data and quality (15 points)

*Here you need to include your review of data, the statistical test
applied to understand the predictors and the response and how are they
correlated. Extensive graph analysis is recommended. Is the data
continuous, or categorical, do any transformation needed? Do you need
dummies?*

## III. Model Development Process (15 points)

*Build a regression model to predict price. And of course, create the
train data set which contains 70% of the data and use set.seed (1023).
The remaining 30% will be your test data set. Investigate the data and
combine the level of categorical variables if needed and drop variables.
For example, you can drop id, Latitude, Longitude, etc.*

## IV. Model Performance Testing (15 points)

*Use the test data set to assess the model performances. Here, build the
best multiple linear models by using the stepwise both ways selection
method. Compare the performance of the best two linear models. Make sure
that model assumption(s) are checked for the final linear model. Apply
remedy measures (transformation, etc.) that helps satisfy the
assumptions. In particular you must deeply investigate unequal variances
and multicollinearity. If necessary, apply remedial methods (WLS, Ridge,
Elastic Net, Lasso, etc.).*

## V. Challenger Models (15 points)

*Build an alternative model based on one of the following approaches to
predict price: regression tree, NN, or SVM or regression model with
alternative variables. Always check the applicable model assumptions.
Apply in-sample and out-of-sample testing, back testing and review the
comparative goodness of fit of the candidate models. Describe step by
step your procedure to get to the best model and why you believe it is
fit for purpose.*

## VI. Model Limitation and Assumptions (15 points)

*Based on the performances on both train and test data sets, determine
your primary (champion) model and the other model which would be your
benchmark model. Validate your models using the test sample. Do the
residuals look normal? Does it matter given your technique? How is the
prediction performance using Pseudo R\^2, SSE, **RMSE**? Benchmark the
model against alternatives. How good is the relative fit? Are there any
serious violations of the model assumptions? Has the model had issues or
limitations that the user must know? (Which assumptions are needed to
support the Champion model?)*

## VII. Ongoing Model Monitoring Plan (5 points)

*How would you picture the model needing to be monitored, which
quantitative thresholds and triggers would you set to decide when the
model needs to be replaced? What are the assumptions that the model must
comply with for its continuous use?*

## VIII. Conclusion (5 points)

*Summarize your results here. What is the best model for the data and
why?*

## Bibliography (7 points)

*Please include all references, articles and papers in this section.*

https://www.openintro.org/data/index.php?data=ames
https://jse.amstat.org/v19n3/decock/DataDocumentation.txt
https://stackoverflow.com/questions/24027605/determine-the-number-of-na-values-in-a-column
https://stackoverflow.com/questions/75084373/how-to-remove-rows-by-condition-in-r
https://stackoverflow.com/questions/72083993/how-to-list-the-unique-categorical-values-of-a-data-frame-column
https://uc-r.github.io/descriptives_categorical
https://bookdown.org/rwnahhas/IntroToR/summarizing-categorical-data.html
https://stackoverflow.com/questions/19410108/cleaning-up-factor-levels-collapsing-multiple-levels-labels



One can fin our preferred bibliography format at the below:

[Harvard referencing
(shef.ac.uk)](https://librarydevelopment.group.shef.ac.uk/referencing/harvard.html)

## Appendix (3 points)

*Please add any additional supporting graphs, plots and data analysis.*
